from workflow.kernel import Kernel, KernelSpec, EDASDataset, OpKernel
import xarray as xa
from edas.process.operation import OpNode, MasterNode
from edas.process.task import TaskRequest
from edas.workflow.data import EDASArray
from typing import List, Tuple
import copy, random, numpy as np
from keras.models import Sequential, Model
from collections import OrderedDict
from edas.process.operation import WorkflowConnector
from edas.collection.agg import Archive
from keras.optimizers import SGD
import keras.backend as K
from keras.callbacks import TensorBoard, History
from workflow.modules.keras_defunct.learning import FitResult, PerformanceTracker, KerasModel


class LayerKernel(OpKernel):
    def __init__( self ):
        Kernel.__init__( self, KernelSpec("layer", "Layer Kernel","Represents a layer in a neural network." ) )
        self.parent = "keras.network"

class NetworkKernel(OpKernel):
    # Parent of (proxy) LayerKernel

    def __init__( self ):
        Kernel.__init__( self, KernelSpec("network", "Network Kernel","Represents a neural network architecture." ) )

    def getModel( self, masterNode: MasterNode ):
        keras_layers = masterNode["layers"]
        keras_model = Sequential()
        for keras_layer in keras_layers:
            keras_model.add( keras_layer )
        return keras_model

    def processInputCrossSection( self, request: TaskRequest, node: OpNode, inputDset: EDASDataset ) -> EDASDataset:
        assert isinstance( node, MasterNode ), "Model kernel must be associated with a Master Node"
        masterNode: MasterNode = node
        layerNodes: List[OpNode] = masterNode.getInputProxies()
        assert len(layerNodes) == 1, "Must have one and only one input layer to network, found {}".format( len(layerNodes) )
        layers, orderedLayerNodes = KerasModel.getLayers( layerNodes[0], inputDset )
        masterNode["layers"] = layers
        masterNode["layerNodes"] = orderedLayerNodes
        return inputDset

class ModelOps:

    @classmethod
    def loadModel(cls, modelPath: str ) -> Model:
        modelData: EDASDataset = EDASDataset.open_dataset( modelPath )
        layersSpec = modelData["layers"]
        assert layersSpec, "Missing levels spec in model data"
        layerNodes = [ OpNode.deserialize(spec) for spec in layersSpec.split(";") ]
        layers = KerasModel.instantiateLayers( layerNodes )
        model = KerasModel.getModel( layers )
        initial_weights = model.get_weights()
        model.compile(loss="mse", optimizer=SGD(), metrics=['accuracy'])
        weights = KerasModel.packWeights( "finalWts", modelData )
        print( "INITIAL WEIGHTS: " + str( initial_weights ) )
        print( "INPUT WEIGHTS: " + str( weights ) )
        model.set_weights( weights )
        return model

class ModelPredictionKernel(OpKernel):

    def __init__( self ):
        Kernel.__init__( self, KernelSpec("model", "Model Kernel","Represents a trained neural network." ) )

    def processVariable(self, request: TaskRequest, node: OpNode, variable: EDASArray ) -> List[EDASArray]:
        modelId = node.getParm( "model", "model" )
        model = ModelOps.loadModel( self.archivePath( modelId, {} ) )
        weights = model.get_weights()
        print( "MODEL WEIGHTS: " + str(weights))
        input_size = weights[0].shape[0]
        input = KerasModel.getNetworkInput( node, variable, input_size )
        return [ KerasModel.map( "predict", model, input ) ]

class BackProjectionMap(OpKernel):

    def __init__( self ):
        Kernel.__init__( self, KernelSpec("backProject", "Back project model node","Generates patterns that maximally activate each output node." ) )

    def processVariable(self, request: TaskRequest, node: OpNode, variable: EDASArray ) -> List[EDASArray]:
        modelId = node.getParm( "model", "model" )
        model: Model = ModelOps.loadModel( self.archivePath( modelId, {} ) )

        out_diff = K.mean((model.layers[-1].output - 1) ** 2)
        grad = K.gradients(out_diff, [model.input])[0]
        grad /= K.maximum(K.sqrt(K.mean(grad ** 2)), K.epsilon())
        iterate = K.function( [model.input, K.learning_phase()], [out_diff, grad] )
        input_img_data = np.zeros( shape=variable.xrArray.shape )

        self.logger.info("Back Projection Map, Iterations:")
        for i in range(20):
            out_loss, out_grad = iterate([input_img_data, 0])
            input_img_data -= out_grad * 0.1
            self.logger.info( str(i) + ": loss = " + str(out_loss) )
        return [ EDASArray( "Back Projection Map", variable.domId, xa.DataArray(input_img_data) ) ]

class TrainKernel(OpKernel):
    def __init__( self ):
        Kernel.__init__( self, KernelSpec("train", "Train Kernel","Train a neural network model." ) )
        self.weights = None
        self.bestFitResult: FitResult = None
        self.tensorboard = TensorBoard( log_dir=Archive.getLogDir(), histogram_freq=0, write_graph=True )
        self.stop_condition = "minValTrain"

    def getModel(self, node: OpNode ) -> Tuple[MasterNode,Model]:
        input_connection: WorkflowConnector = node.inputs[0]
        master_node = input_connection.connection
        assert isinstance( master_node, MasterNode ), "Training Kernel is not connected to a network!"
        return master_node, KerasModel.getModel( master_node["layers"] )

    def buildLearningModel(self, node: MasterNode, model: Model ):
        lr = node.getParm( "lr", 0.002 )
        decay = node.getParm( "decay", 0.002 )
        momentum = node.getParm( "momentum", 0.9 )
        nesterov = bool(node.getParm( "nesterov", False ))
        sgd = SGD( lr=lr, decay=decay, momentum=momentum, nesterov=nesterov )
        model.compile(loss=node.getParm("loss","mse"), optimizer=sgd, metrics=['accuracy'])
        if self.weights is not None:
            model.set_weights(self.weights)

    def fitModel(self, master_node: MasterNode, train_node: OpNode, model: Model, inputDset: EDASDataset, performanceTracker: PerformanceTracker) -> FitResult:
        batch = master_node.getParm( "batch", 200 )
        epochs = master_node.getParm( "epochs", 600 )
        vf = master_node.getParm( "vf", 0.2 )

        shuffle = master_node.getParm( "shuffle", True )
        inputData = KerasModel.getTrainingData( master_node, inputDset, 1 )
        targetData = KerasModel.getTargetData( train_node, inputDset, 1 )
        initial_weights = model.get_weights()
        history: History = model.fit( inputData[0].nd, targetData[0].nd, callbacks=[self.tensorboard,performanceTracker], verbose=0,
                                      batch_size=batch, epochs=epochs, validation_split=vf, shuffle=shuffle,  )
        return self.updateHistory( history, initial_weights, performanceTracker )

    def buildResultDataset(self, inputDset: EDASDataset, train_node: OpNode )-> EDASDataset:
        result = self.bestFitResult
        master_node, model = self.getModel(train_node)
        arrays = OrderedDict()
        loss_coord = ( "steps", range(result.train_loss_history.shape[0]) )
        arrays["loss"] =     EDASArray( "loss" ,     None, xa.DataArray( result.train_loss_history, coords=( loss_coord, ) ), [] )
        arrays["val_loss"] = EDASArray( "val_loss" , None, xa.DataArray( result.val_loss_history,   coords=( loss_coord, ) ), [] )
        attrs = copy.deepcopy(inputDset.attrs)
        attrs["loss"] = result.train_loss
        attrs["val_loss"] = result.val_loss
        attrs["nEpocs"] = result.nEpocs
        attrs["nInstances"] = result.nInstances
        attrs["merge"] = "min:val_loss"
        attrs["layers"] = ";".join( [ op.serialize() for op in master_node["layerNodes"] ] )
        arrays.update( KerasModel.unpackWeights( "initWts", result.initial_weights ) )
        arrays.update( KerasModel.unpackWeights( "finalWts", result.final_weights ) )
        attrs["nlayers"] = int( len(result.final_weights)/2 )
        model.set_weights(result.final_weights)
        inputData = KerasModel.getTrainingData(master_node, inputDset, 1)
        targetData = KerasModel.getTargetData(train_node, inputDset, 1)
        arrays["prediction"] = KerasModel.map( "prediction", model, inputData[0] )
        arrays["target"] = targetData[0]
        rv = EDASDataset( arrays, attrs )
        return rv

    def updateHistory( self, history: History, initial_weights: List[np.ndarray], performanceTracker: PerformanceTracker ) -> FitResult:
        if performanceTracker.nEpoc > 0:
            if not self.bestFitResult or ( performanceTracker.minValLoss < self.bestFitResult.val_loss ):
                self.bestFitResult = FitResult.new( history, initial_weights, performanceTracker.getWeights(), performanceTracker.minTrainLoss, performanceTracker.minValLoss, performanceTracker.nEpoc )
        return self.bestFitResult

    def getHistoryDataArray(self, history: History, id: str, nEpochs: int )-> EDASArray:
        data = xa.DataArray(history.history[id], coords=[ range( nEpochs ) ], dims=["epochs"])
        return EDASArray( id, None, data )

    def getWtsArray(self, array: np.ndarray, id: str )-> EDASArray:
        coords = [ ('inputs',range(array.shape[0]) ), ('nodes',range(array.shape[1]) ) ] if array.ndim == 2 else [ ('nodes',range(array.shape[0]) ) ]
        return EDASArray( id, None, xa.DataArray( array, coords=coords ) )

    def processInputCrossSection( self, request: TaskRequest, train_node: OpNode, inputDset: EDASDataset ) -> EDASDataset:
        self.reseed()
        nIterations = train_node.getParm( "iterations", 1 )
        self.logger.info( "Executing fit-model {} times".format(nIterations) )
        val_loss_values = []
        for idx in range( nIterations ):
            performanceTracker = PerformanceTracker( self.stop_condition )
            master_node, model = self.getModel( train_node )
            self.buildLearningModel( master_node, model )
            self.fitModel( master_node, train_node, model, inputDset, performanceTracker )
            val_loss_values.append( performanceTracker.minValLoss )
        self.logger.info( "Worker training results: val losses = " + str(val_loss_values) )
        return self.buildResultDataset(inputDset, train_node)

    def reseed(self):
        seed = random.randint(0, 2 ** 32 - 2)
        np.random.seed(seed)
